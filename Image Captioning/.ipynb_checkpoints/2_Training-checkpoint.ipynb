{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our CNN-RNN model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.99s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.89s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 877/414113 [00:00<01:35, 4333.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:37<00:00, 4246.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "#Tune the hyperparameter\n",
    "batch_size = 256          \n",
    "vocab_threshold = 3        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size = 300           \n",
    "hidden_size = 512         \n",
    "num_epochs = 3            \n",
    "save_every = 1             \n",
    "print_every = 100         \n",
    "log_file = 'training_log.txt'      \n",
    "\n",
    "#Define transformations on the images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.RandomCrop(224),                      \n",
    "    transforms.RandomHorizontalFlip(),               \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # values from the documentation of the pre-trained ResNet50\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params = params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/1618], Loss: 3.7552, Perplexity: 42.74317\n",
      "Epoch [1/3], Step [200/1618], Loss: 3.2896, Perplexity: 26.8325\n",
      "Epoch [1/3], Step [300/1618], Loss: 3.2398, Perplexity: 25.5281\n",
      "Epoch [1/3], Step [400/1618], Loss: 2.8949, Perplexity: 18.0821\n",
      "Epoch [1/3], Step [500/1618], Loss: 2.7128, Perplexity: 15.0716\n",
      "Epoch [1/3], Step [600/1618], Loss: 2.8047, Perplexity: 16.5227\n",
      "Epoch [1/3], Step [700/1618], Loss: 2.6423, Perplexity: 14.0456\n",
      "Epoch [1/3], Step [800/1618], Loss: 2.5766, Perplexity: 13.1523\n",
      "Epoch [1/3], Step [900/1618], Loss: 2.5084, Perplexity: 12.2852\n",
      "Epoch [1/3], Step [1000/1618], Loss: 2.4567, Perplexity: 11.6657\n",
      "Epoch [1/3], Step [1100/1618], Loss: 2.5298, Perplexity: 12.5516\n",
      "Epoch [1/3], Step [1200/1618], Loss: 3.2065, Perplexity: 24.6922\n",
      "Epoch [1/3], Step [1300/1618], Loss: 2.3000, Perplexity: 9.97382\n",
      "Epoch [1/3], Step [1400/1618], Loss: 2.2814, Perplexity: 9.79066\n",
      "Epoch [1/3], Step [1500/1618], Loss: 2.3038, Perplexity: 10.0123\n",
      "Epoch [1/3], Step [1600/1618], Loss: 2.5859, Perplexity: 13.2752\n",
      "Epoch [2/3], Step [100/1618], Loss: 2.6707, Perplexity: 14.45047\n",
      "Epoch [2/3], Step [200/1618], Loss: 2.2184, Perplexity: 9.19253\n",
      "Epoch [2/3], Step [300/1618], Loss: 2.2132, Perplexity: 9.14533\n",
      "Epoch [2/3], Step [400/1618], Loss: 2.1789, Perplexity: 8.83676\n",
      "Epoch [2/3], Step [500/1618], Loss: 2.1435, Perplexity: 8.52928\n",
      "Epoch [2/3], Step [600/1618], Loss: 2.1240, Perplexity: 8.36450\n",
      "Epoch [2/3], Step [700/1618], Loss: 2.0753, Perplexity: 7.96720\n",
      "Epoch [2/3], Step [800/1618], Loss: 2.1452, Perplexity: 8.54376\n",
      "Epoch [2/3], Step [900/1618], Loss: 2.2481, Perplexity: 9.46936\n",
      "Epoch [2/3], Step [1000/1618], Loss: 2.1902, Perplexity: 8.9368\n",
      "Epoch [2/3], Step [1100/1618], Loss: 2.0724, Perplexity: 7.94423\n",
      "Epoch [2/3], Step [1200/1618], Loss: 2.1393, Perplexity: 8.49317\n",
      "Epoch [2/3], Step [1300/1618], Loss: 2.0542, Perplexity: 7.80054\n",
      "Epoch [2/3], Step [1400/1618], Loss: 2.4237, Perplexity: 11.2879\n",
      "Epoch [2/3], Step [1500/1618], Loss: 1.9742, Perplexity: 7.20087\n",
      "Epoch [2/3], Step [1600/1618], Loss: 2.0530, Perplexity: 7.79139\n",
      "Epoch [3/3], Step [100/1618], Loss: 2.0765, Perplexity: 7.976417\n",
      "Epoch [3/3], Step [200/1618], Loss: 1.9315, Perplexity: 6.90019\n",
      "Epoch [3/3], Step [300/1618], Loss: 2.3270, Perplexity: 10.2468\n",
      "Epoch [3/3], Step [400/1618], Loss: 2.2840, Perplexity: 9.81558\n",
      "Epoch [3/3], Step [500/1618], Loss: 2.0947, Perplexity: 8.12345\n",
      "Epoch [3/3], Step [600/1618], Loss: 2.0830, Perplexity: 8.02850\n",
      "Epoch [3/3], Step [700/1618], Loss: 2.1181, Perplexity: 8.31542\n",
      "Epoch [3/3], Step [800/1618], Loss: 1.9287, Perplexity: 6.88067\n",
      "Epoch [3/3], Step [900/1618], Loss: 2.1563, Perplexity: 8.63924\n",
      "Epoch [3/3], Step [1000/1618], Loss: 2.2061, Perplexity: 9.0803\n",
      "Epoch [3/3], Step [1100/1618], Loss: 1.9919, Perplexity: 7.32955\n",
      "Epoch [3/3], Step [1200/1618], Loss: 2.2076, Perplexity: 9.09407\n",
      "Epoch [3/3], Step [1300/1618], Loss: 2.1131, Perplexity: 8.27398\n",
      "Epoch [3/3], Step [1400/1618], Loss: 2.2253, Perplexity: 9.25657\n",
      "Epoch [3/3], Step [1500/1618], Loss: 1.8946, Perplexity: 6.64962\n",
      "Epoch [3/3], Step [1600/1618], Loss: 2.0032, Perplexity: 7.41279\n",
      "Epoch [3/3], Step [1618/1618], Loss: 1.9929, Perplexity: 7.33679"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
